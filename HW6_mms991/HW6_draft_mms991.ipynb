{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# fbb skeleton notebook for PUI2017 HW6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Could not find lib geos_c or load any of its variants ['/Library/Frameworks/GEOS.framework/Versions/Current/GEOS', '/opt/local/lib/libgeos_c.dylib'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e109b8f28ce5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#s = json.load( open(os.getenv('PUI2016')+'/fbb_matplotlibrc.json') )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sherlock/anaconda/lib/python3.6/site-packages/geopandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeoSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeodataframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_postgis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sherlock/anaconda/lib/python3.6/site-packages/geopandas/geoseries.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyproj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseGeometry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sherlock/anaconda/lib/python3.6/site-packages/shapely/geometry/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \"\"\"\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCAP_STYLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJOIN_STYLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masShape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masPoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sherlock/anaconda/lib/python3.6/site-packages/shapely/geometry/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffinity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maffine_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoordinateSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWKBReadingError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWKTReadingError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sherlock/anaconda/lib/python3.6/site-packages/shapely/coords.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mctypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbyref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_double\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_uint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlgeos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mValidating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sherlock/anaconda/lib/python3.6/site-packages/shapely/geos.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;34m'/opt/local/lib/libgeos_c.dylib'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             ]\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0m_lgeos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'geos_c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malt_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mfree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sherlock/anaconda/lib/python3.6/site-packages/shapely/geos.py\u001b[0m in \u001b[0;36mload_dll\u001b[0;34m(libname, fallbacks, mode)\u001b[0m\n\u001b[1;32m     54\u001b[0m         raise OSError(\n\u001b[1;32m     55\u001b[0m             \"Could not find lib {0} or load any of its variants {1}.\".format(\n\u001b[0;32m---> 56\u001b[0;31m                 libname, fallbacks or []))\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0m_lgeos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Could not find lib geos_c or load any of its variants ['/Library/Frameworks/GEOS.framework/Versions/Current/GEOS', '/opt/local/lib/libgeos_c.dylib']."
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import os\n",
    "import json\n",
    "import geopandas as gp\n",
    "\n",
    "#s = json.load( open(os.getenv('PUI2016')+'/fbb_matplotlibrc.json') )\n",
    "#pl.rcParams.update(s)\n",
    "#if os.getenv(\"PUIDATA\") is None:\n",
    "#    print (\"$PUIDATA to point to set PUIdata dir\")\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "I am using geopanda. that is **not required** for this particular exercise, but geopanda works with geospacial data: the shape files that we get from pluto for example.\n",
    "\n",
    "PLEASE REMEMBER: download your data in a reproducible way, seed your random functions if you need to use any, label your axes clearly, captions for each figure that explains what is shown, and what is noticeable about it, comment your code, use PEP8!\n",
    "\n",
    "\n",
    "** An interesting urban question is \"can we measure and predict energy use based on observables that are easily acquired\". For example the urban observatory at CUSP can monitor lights: they are a relatively easy observable. All you need is a camera, and a pipeline to process your data. But how does the light coming from a window relate to the total energy consumption? We generally postulate that light is a proxy for occupancy, and that occupancy is a good predictor of energy consumption.**\n",
    "\n",
    "** So let's test if the last link holds. If we have data on the _energy consumed by a building_ how well does that relate to the _number of units_ in the building?**\n",
    "\n",
    "** Data on energy consumption can be found here for the city of NY https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/rgfe-8y2z  **\n",
    "\n",
    "** Either obtain the data through the API or download the csv file, and move it to $PUIDATA**\n",
    "\n",
    "** However this datasets does not have the number of units. We can find that in the [Pluto dataset](https://www1.nyc.gov/site/planning/data-maps/open-data/dwn-pluto-mappluto.page).**\n",
    "\n",
    "** Reading in the Pluto data for manhattan, which will give me the number of units ber building   Manhattan/MNMapPLUTO.shp ** Note that you should keep all the files in the original MNMapPluto zipped folder (not just the .shp file) for geopandas to read the data in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T21:53:07.869072",
     "start_time": "2017-10-19T21:52:36.962480"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrg = gp.GeoDataFrame.from_csv(os.getenv(\"PUIDATA\") + \"/Energy_and_Water_Data_Disclosure_for_Local_Law_84__2013_.csv\")\n",
    "#os.system(\"unzip -d $PUIDATA $PUIDATA/mn_mappluto_16v2.zip\")\n",
    "bsize = gp.GeoDataFrame.from_file(os.getenv(\"PUIDATA\") + \"/MNMapPLUTO.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:18:45.706267",
     "start_time": "2017-10-19T15:18:45.702833"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure you clean up your data and throw away columns you do not need!\n",
    "nrg = nrg[['BBL', 'Reported NYC Building Identificaiton Numbers (BINs)', 'Street Number',\n",
    "       'Street Name', 'Borough', 'Postcode', 'Site EUI(kBtu/ft2)', 'Source EUI(kBtu/ft2)',\n",
    "       'Reported Property Floor Area (Building(s)) (ft²)','DOF Number of Buildings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bsize = bsize[['BBL', 'UnitsRes', 'UnitsTotal', 'NumFloors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bsize.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed, coming up with a sensible model generally requires domain expertise. However, if the data you are investigating shows \"obvious patterns\", for example if two of the variable look like a line when plotted one against the other, then those patterns (correlations) may help you finding reasonable models for the data.\n",
    "\n",
    "Explore your data, starting with a scatter matrix. \n",
    "A scatter matrix is a plot of all variables in your data against all other variables: \n",
    "each pair of variables is a subplot in the plot matrix. The diagonal line then would be a plot of a variable against itself, which is useless, so it is usually substituted by a histogram of that variable (or sometimes a KDE, which is basically a smooth histogram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:22:00.187105",
     "start_time": "2017-10-19T15:21:54.857349"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#try make a scatter plot of nrg. Few columns will plot - only those that have numerical values. \n",
    "#Pandas will ignore the other ones\n",
    "\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "scatter_matrix (nrg, s=300, figsize=(16, 16));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Figure 1: scatter matrix of all numerical values in the files. ... comments on what you see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "This kind of plot shows correlations between variables, but it will also show me what can and cannot be plotted trivially in my dataset. Here only a few columns can be plotted: those that contain only numbers (and possibly NaN's), but most columns contain rows that cannot be converted to float (e.g. entries like 'See Primary BBL' in several rows for the energy dataframe 'Site EUI(kBtu/ft2) ' column) , so Pandas refuses to plot them, cause it does not know what to do with those entries. The columns I am interested in are in fact u'Site EUI(kBtu/ft2)' which is a measure of the energy consumed PER SQ FOOT by a building, and then the building area: for eg. u'Reported Property Floor Area (Building(s)) (ft²)'. Neither gets plotted: I need to remove all values that cannot convert to float in order to use the columns and convert them to float arrays and do math with them.\n",
    "\n",
    "You can use pd.to_numeric() which is a function that transforms values to float (when possible). The default behavior of this function is to throw an error if it encounters a value that it cannot convert. That behavior can be modified with the \"error\" keyword, by setting it to \"coerce\". Please look at the function documentation to understand the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# use pd.to_numeric to convert strings to numeric values, \n",
    "##check that your conversion worked: e.g.\n",
    "nrg['Site EUI(kBtu/ft2)'] = pd.to_numeric(nrg['Site EUI(kBtu/ft2)'], errors=\"coerce\")\n",
    "nrg['Reported Property Floor Area (Building(s)) (ft²)']=pd.to_numeric(nrg['Reported Property Floor Area (Building(s)) (ft²)'],errors='coerce')\n",
    "\n",
    "# print (nrg['Site EUI(kBtu/ft2)'].astype(float))\n",
    "# print (nrg['Reported Property Floor Area (Building(s)) (ft²)'].astype(float))\n",
    "\n",
    "#[...] do this for all columns you care about in both datasets. \n",
    "#Nan's are ok, but you must not get an error when you try the conversion\n",
    "#the Pluto data is much better at not using spurious entries for numerical value columns. \n",
    "#but check you can use the columns you want\n",
    "\n",
    "# bsize.BBL.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:45:44.349333",
     "start_time": "2017-10-19T15:45:44.345266"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop everything you do not need to lighten the memory load on your machine! this is important!! \n",
    "#this file has a lot of columnsm most of them you will not need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrg = nrg[['BBL', 'Site EUI(kBtu/ft2)','Reported Property Floor Area (Building(s)) (ft²)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:46:50.865485",
     "start_time": "2017-10-19T15:46:50.858740"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#How many missing values?\n",
    "indx = np.isnan(nrg['Site EUI(kBtu/ft2)']).sum()\n",
    "print (\"invalid Site EUI entries changed to NaN %d\"%sum(indx))\n",
    "#do it for however many columns you need\n",
    "indx2 = np.isnan(nrg['Reported Property Floor Area (Building(s)) (ft²)']).sum()\n",
    "print (\"invalid Reported Property Floor Area entries changed to NaN %d\"%sum(indx2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "** MERGE THE DATASETS**\n",
    "look at the syntax for pandas.merge - this will be incredibly useful to you in all future data problem where you use Pandas and data aggregation is really at the heart of urban science!\n",
    "\n",
    "TO DO IT WE NEED A COMMON COLUMN: the building id, BBL is in both files. However the name of this variable (column) in the Energy dataset is 'NYC Borough, Block, and Lot (BBL)'. \n",
    "You can rename the column, create a whole new column 'BBL' in the energy dataset to pass it to the 'on' keyword argument of the merge pandas method: pd.merge(..... on=['BBL']) will use the common column 'BBL' to join the information from the 2 datasets for each BBL value (check out the complete syntax!). You can also say pd.merge(..., right_on=BBL, left_on=\"NYC Borough, Block, and Lot (BBL)'). Always make sure though that the data _type_ is the same:printing the columns the content may look the same but your computer wont know it is the same if it is a different data type (e.g. '1' string is not the same as 1 integer'. '1' == 1 returns False)! both have to be integers, both strings, or whatever, but the same, or you will not be able to merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:47:20.434273",
     "start_time": "2017-10-19T15:47:20.426426"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(bsize.BBL.values[0]), (nrg.BBL.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:47:32.313527",
     "start_time": "2017-10-19T15:47:32.267121"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bblnrgdata = pd.merge(nrg, bsize, on='BBL').dropna()\n",
    "bblnrgdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bblnrgdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:48:56.747639",
     "start_time": "2017-10-19T15:48:52.459258"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Now the scatter matrix plot should show more columns.\n",
    "scatter_matrix (bblnrgdata, s=30, figsize=(16, 16));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:49:16.596228",
     "start_time": "2017-10-19T15:49:16.591281"
    }
   },
   "source": [
    "Figure 2: scatter matix of final dataset (please describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "once you have the dataframe with all the info you want, you want to plot Energy vs Number of Units in the Building.  **Energy TOTAL, not per sq ft...** Here you can choose what you think makes more sense for the number of units: all units, residential units... \n",
    "\n",
    "**Make a  scatter plot first of Energy vs Units. It will look really bad be cause all the datapoints are at very low Unit numbers while the Unit number range actually goes up to 8000. **\n",
    "\n",
    "\n",
    "Make a second plot that zooms into the region where most points are by cutting your x and y axis plotted: e,g, use xlim=(1000,1e10), ylim=(1,1000), where the numbers to be plugged in depend on exactly what you chose to plot\n",
    "\n",
    "I left my plots below as guidance. **Remember, each plot needs a descriptive caption, and axis labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bblnrgdata['TotEnergy'] = bblnrgdata['Site EUI(kBtu/ft2)'] * bblnrgdata['Reported Property Floor Area (Building(s)) (ft²)']\n",
    "bblnrgdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bblnrgdata.TotEnergy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first scatter plot\n",
    "fig = pl.figure(figsize=(15,5))\n",
    "fig.add_subplot(121)\n",
    "x = bblnrgdata.TotEnergy.values\n",
    "y = bblnrgdata.UnitsTotal.values\n",
    "pl.scatter(x= x, y= y)\n",
    "pl.title(\"Total Energy Use by Number of Units\")\n",
    "pl.xlabel(\"Total # of Units (thousands)\")\n",
    "pl.ylabel(\"Total Energy Use Intensity (EUI)(kBtu)\")\n",
    "\n",
    "fig.add_subplot(122)\n",
    "pl.scatter(x= x, y= y)\n",
    "pl.title(\"Total Energy Use by Number of Units [zoomed in]\")\n",
    "pl.xlabel(\"Total # of Units (thousands)\")\n",
    "pl.ylabel(\"Total Energy Use Intensity (EUI)(kBtu)\")\n",
    "pl.ylim(0,2000)\n",
    "pl.xlim(0,0.51e10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** IMPORTANT NOTE ABOUT LOGS AND LOG PLOTS **\n",
    "in class we talked about logs when we talked about likelihood: often we prefer working with the log(likelihood) instead of the likelihood, and since all problems involving likelihood are about maximization (find the maximum likelihood to find the best fit parameters) and the log is a MONOTONIC function (log(x) grows when x grows, and gets smaller when x gets smaller) the maximum of the likelihood of a model with be in the same place as the maximum of the log(likelihood). \n",
    "\n",
    "Another great thing about logarithm: **when the points in a plot all look scrunched against the axis **\n",
    "**try to make a log plot instead**. In pandas you enable that with the keyword 'loglog' : bblnrgdata.plot(..... loglog=True)\n",
    "\n",
    "This will compress the high  x and high  y values, and expand the small x and small y values. \n",
    "\n",
    "\n",
    "\n",
    "NOTICE THAT YOU WILL STILL HAVE TO CUT YOUR DATASET! in my data I had a lot of energy datapoints that were exactly 0. I removed these \"outliers\" which I think are truly outliers in the sense that they are misreported numbers. You can remove the data that have nrg==0 (or nrg < some sensible threshold choice) by indexing your array: something like bblnrgdata_cut = bblnrgdata[bblnrgdata.nrg>1000] - see below.\n",
    "\n",
    "Also I removed the buildings with several thousand units. Points like that at the edge of my range would have a lot of \"LEVERAGE\", however they are not spurious entries like the 0, which I believe are missing values, or perhaps abandoned lots. These are good datapoint that I need to throw away functionally for my analysis to work, but that should be stated clearly in your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you may need to change the name of this column under some versions of pandas\n",
    "#(ft²) may throw an error due to the funny character depending on encoding\n",
    "bblnrgdata['Reported Property Floor Area (Building(s))'] = bblnrgdata['Reported Property Floor Area (Building(s)) (ft²)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T17:28:47.312217",
     "start_time": "2017-10-19T17:28:45.424304"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# I have the energy per sq foot in the data, I need the total energy though\n",
    "bblnrgdata['nrg'] = ...\n",
    "\n",
    "bblnrgdataCut = bblnrgdata[(bblnrgdata.nrg > 1000) * (bblnrgdata.UnitsTotal>=10) * \n",
    "                           (bblnrgdata.UnitsTotal<1000)]\n",
    "\n",
    "ax = bblnrgdataCut.plot(kind='scatter', y='nrg', x='UnitsTotal', \n",
    "                   marker='o',  figsize=(16, 14), loglog=True)\n",
    "yl = ax.set_xlabel(\"Number of Units in Building\", fontsize=20)\n",
    "xl = ax.set_ylabel(\"Energy consumption per building (kBtu)\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now fit a line through the data. you can use whatever you want to do it: statsmodels, scipy, any other package, or write your own minimization of the residuals\n",
    "\n",
    "## BUT REMEMBER: we see hints of a linear relation in log space! so we want to fit a line to the log of our variables, not the variables themselves:\n",
    "if you used statsmodels it would look something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "X = sm.add_constant(np.log10(x))\n",
    "linmodel = sm.OLS(np.log10(y), X, missing='drop').fit() # ignores entires \n",
    "                                                     # where x or y is NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## choose  which is your DEPENDENT and which is your INDEPENDENT variable. \n",
    "which is the \"logical\" IV: what are we assuming depends on what? energy on size of building or building on size of energy... discuss this but also test both fits, energy vs size and size vs energy. how can you compare these models? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "\n",
    "\n",
    "1. **Fit a line** to Units vs Energy. The independent variable in this problem should be number of units, but try fit both Unity to energy and energy to unit.\n",
    "2. **Fit a line** to Energy vs Units.\n",
    "3. **Evaluate which is better by calculating the chi square**.  Can you compare these models with the likelihood ratio test? (hint: are they nested??) I provide a function to calculate the chi square or you can write your own.\n",
    "\n",
    "    The function is :\n",
    "    \n",
    "    chisq = $\\sum_i \\frac{(model(x_i) - data(x_i))^2 }{ error_i^2}$\n",
    "    \n",
    "    where the sum is over all datapoints, \n",
    "    \n",
    "    for the i-th value  model($x_i$) is the prediction, data($x_i$) the observation,\n",
    "    \n",
    "    and $error_i$ is $\\sqrt{data(x_i)}$\n",
    "    (but remember you worked in log space! What are the proper errors?? see next point)\n",
    "    \n",
    "4. *Assume Poisson statistics* for the errors on the *independent variable*. Poisson statistics means your uncertainty is the square root of your measurement. I wrote down the formula for the errors. Please explain why I wrote it as I did.\n",
    "\n",
    "5. **Fit a 2nd degree polynomial** to the Units vs Energy (with statsmodels.formulae.api.ols() for example passing the formula for a parabola, like we did in class. The formula for a 2nd deg polynomial is \n",
    "    $y = ax^2 + bx + c$ .\n",
    "\n",
    "6. **Compare the Units vs Energy line fit and the Units vs Energy 2-nd degree polynomial fit with the Likelihood ratio test**. The formula is:\n",
    "    \n",
    "    LR  =  -2 * log(Likelihood_Model1 / Likelihood_Model2) \n",
    "\n",
    "    or equivalently\n",
    "        \n",
    "    LR  =  -2 * (logLikelihood_Model1 - logLikelihood_Model2) \n",
    "    \n",
    "    where Model1 is the _least complex_ (fewer parameters).\n",
    "    \n",
    "    Th logLikelihood can be extracted from the model summary when using statsmodels. (Model1.llf)\n",
    "    \n",
    "    Compare this LR statistics to a chi sq table (for example http://passel.unl.edu/Image/Namuth-CovertDeana956176274/chi-sqaure%20distribution%20table.PNG) and say if *at alpha = 0.05* Model1 is preferable to Model2. The LR is chi^2 distributed with number of degrees of freedom $N_{DOF}$ = parameters_Model2 - parameters_Model1\n",
    "    \n",
    "    \n",
    "    Also if you used statsmodels for the fit you can use the \n",
    "    compare_lr_test() method of your fit and verify you got the right answer.  Use the method compare_lr_test() of the most complex model of the 2 and pass it the result of stats models for the simpler fit \n",
    "    (e.g. smf.ols(formula = ...).fit().compare_lr_test(sm.OLS(...).fit()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T22:12:56.384129",
     "start_time": "2017-10-19T22:12:56.371558"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# fits and plots here\n",
    "# your plots should show datapoints (as scatter plot) and models (as lines)\n",
    "# Make sure your model for the quadratic fit looks right: dont just join the points, you need to sort them first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# my OLS summary. \n",
    "# Yours may be somewhat different depending on how you cut the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember your captions etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chi2(data, model, errors = None):\n",
    "    '''Calculates the chi sq given data, model and errors\n",
    "    Arguments:\n",
    "    data: series of datapoints (endogenous variable)\n",
    "    model: series of predicted values corresponding to the observed data\n",
    "    errors: serie of errors (optional). \n",
    "    If errors are not passes all errors are set to 1\n",
    "    '''\n",
    "    if errors is None:\n",
    "        errors = np.ones_like(data)\n",
    "    if data.shape == model.shape and data.shape == errors.shape:\n",
    "        return (((data - model)**2) / errors**2).sum()\n",
    "    else: \n",
    "        print ('''ERROR:\n",
    "must pass arrays of identical dimension for data, model and (optional) error)''')\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T22:17:25.699044",
     "start_time": "2017-10-19T22:17:25.672033"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Assume that there is error in the reported energy. \n",
    "## but that is the product of two measured qusntities, each of which will have errors. \n",
    "## The minimum error is the squareroot of the value\n",
    "\n",
    "#Below I am writing out some uncertainties - please explain the functional form that I used.\n",
    "#errors on the measured quantities\n",
    "errorsnrg = np.sqrt((bblnrgdataCut['Reported Property Floor Area'])**2 +\\\n",
    "                (bblnrgdataCut['Site EUI(kBtu/ft2)']**2))\n",
    "\n",
    "## Assume count statistics in the number of units as well\n",
    "errorsunits = np.sqrt(bblnrgdataCut.UnitsTotal)\n",
    "\n",
    "#These errors need to be propagated properly through the log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "propagation of errors on the log:https://en.wikipedia.org/wiki/Propagation_of_uncertainty#Linear_combinations\n",
    "\n",
    "$f=a\\log _{10}(A)$\n",
    "\n",
    "$\\sigma _{f}\\approx \\left(a{\\frac {\\sigma _{A}}{A\\ln(10)}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errorsInLogNrg = np.abs(errorsnrg / bblnrgdataCut.nrg / np.log(10))\n",
    "errorsInLogUnits = np.abs(errorsunits / bblnrgdataCut.UnitsTotal / np.log(10))\n",
    "\n",
    "bblnrgdataCut['errorsnrg'] = errorsInLogNrg\n",
    "bblnrgdataCut['errorsunits'] = errorsInLogUnits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print (\"LR : \", -2 * (-linemodel.llf - (-curvemodel.llf)))\n",
    "print (\"LR from statsmodels:\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Extra credit 1: calculate and plot the likelihood surface\n",
    "Create a function that minimizes the residuals:\n",
    "\n",
    "the residuals are the sum of the differences between data and model: in the case of a line fit model. Use the same function you created for the chi^2 test.\n",
    "\n",
    "You should sum over each datapoints the residuals squared, which should look something like\n",
    "\n",
    "(np.log(bblnrgdatacut.nrg) - np.log(bblnrgdatacut.UnitsTotal)*a+b )^2 / errors^2\n",
    "\n",
    "where a and b are the parameters returned by the line fitter. \n",
    "\n",
    "For each data point you can calculate the model at different values : for example in a range B = np.arange (-100, 100, 1) for the intercept, and A = np.arange(-50.0, 50.0, 0.5) for the slope.\n",
    "\n",
    "\n",
    "You can write it as a nested for loop (or challenge yourself and vectorize it!) with a loop inside another ranging all poissible combinations of the 2 variables (i use enumerate to get both an index from 0 to the size of my array, which i assign to i (and j) and the value of the array at that index - look up the syntax!):\n",
    "\n",
    "\n",
    "Lsurface = np.zeros((len(A), len(B)))\n",
    "for i,a in enumerate(A):\n",
    "    for j,b in enumerate(B):\n",
    "         Lsurface[i][j] = np.nansum(residuals(a,b,data,errors)) .....\n",
    "\n",
    "this gives you a 2D array that represents your likelihood surface! What we do to find a good fit is find the minimum (lowest point) on this surface.\n",
    "You can plot a surface (a 2D array) with pl.imshow(Lsurface) as a \"heatmap\" but when you do that you will find that the plot is very uninformative. just like you did before with the data, plot the log of it (pl.imshow(np.log(Lsurface)). Also make sure your x and y axes tick numbers represent the range of values, not the cell index, which is the default for imshow. Inputting your data in the cell below should give a plot similar to mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(10,10))\n",
    "pl.title (\"log likelihood surface\", fontsize = 22)\n",
    "pl.imshow(np.log(Lsurface), extent = [-50,50,100,-100], aspect=0.5)\n",
    "pl.xlabel('slope', fontsize = 22)\n",
    "pl.ylabel('intercept', fontsize = 22)\n",
    "pl.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## EXTRA CREDIT: get creative with the dataset. can you make an insigntful plot to show any structure in the data?\n",
    "\n",
    "below I am mapping the building age to a colormap and the ratio of total to residential units to the size of the datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bblnrgdata['YearBuilt'][bblnrgdata['YearBuilt']<1800]=1800\n",
    "\n",
    "bblnrgdata.plot(kind='scatter',x='nrg',y='UnitsTotal', \n",
    "                fontsize=22, colormap='gist_rainbow', alpha = 1, \n",
    "                marker='o',  figsize=(16, 14), loglog=True,  \n",
    "                xlim=(1000,1e11), ylim=(1,1000), \n",
    "                c=bblnrgdata['YearBuilt']-1900, \n",
    "                s=bblnrgdata['UnitsTotal']/bblnrgdata['UnitsRes']*100)\n",
    "pl.title('Color maps Age in years, Size maps tital/residential units', fontsize=18)\n",
    "pl.ylabel(\"total number of units\", fontsize=22)\n",
    "pl.xlabel(\"total energy consumption (kBtu)\", fontsize=22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "116px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "323px",
    "left": "0px",
    "right": "617.333px",
    "top": "130px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
